environment:
  # General settings
  name: dev-me # name of the environment; drives the naming of kubernetes cluster and nodes, host containers etc
  base-dir: ${PWD}/.local # where kubernetes logs/storage, and various config files are stored
  expand-base-dir-vars: true # set to false if you use absolute path for base-dir; true if you use relative/expanded path

  # At the moment, KinD on macOS has been tested, with a docker-compatible runtime
  # Podman has been tried out but it is not a first-class citizen with KinD so it is not supported as of yet
  # Linux has not been tested yet
  provider:
    name: kind # provider for kubernetes clusters, must be kind for now
    runtime: docker # docker or podman for container runtime

  kubernetes:
    api-port: 6443 # port for the API server, will be exposed to the host machine as local-ip:api-port
    image: kindest/node # image for the kind nodes
    # renovate: datasource=docker depName=kindest/node
    tag: v1.33.1  # tag for the kind image

  nodes: # common settings for all kubernetes nodes
    servers: 1 # number of control-plane servers/masters
    workers: 1 # number of worker nodes
    allow-scheduling-on-control-plane: true # whether to allow scheduling on control plane nodes even when workers exist
    internal-components-on-control-plane: true # whether to force internal components (cert-manager, nginx-ingress, registry) to run only on control-plane nodes

    labels: # optional: custom labels to apply to nodes
      control-plane: # labels for ALL control-plane nodes (applied to each)
        tier: "control"
        environment: "dev"
      worker: # labels for ALL worker nodes (applied to each)
        tier: "compute"
        environment: "dev"
      # Alternative: specify labels per individual node (overrides global labels)
      # individual:
      #   control-plane-0: # labels for first control-plane node
      #     tier: "control"
      #     zone: "us-west-1a"
      #   control-plane-1: # labels for second control-plane node  
      #     tier: "control"
      #     zone: "us-west-1b"
      #   worker-0: # labels for first worker node
      #     tier: "compute"
      #     zone: "us-west-1a"
      #   worker-1: # labels for second worker node
      #     tier: "compute" 
      #     zone: "us-west-1b"

  local-ip: 192.168.0.10 # local ip, mapping to the host IP to use for DNS resolution and wildcard certificates
  local-domain: dev.me # a domain name to use for custom dns resolution and wildcard certificates 
  local-lb-ports: # list of ports to expose on the load balancer side, mapping to the host machine
    - 80 # http port for nginx ingress controller
    - 443 # https port for nginx ingress controller

  registry:
    name: cr # name, to be used in the final url for the registry, i.e. <registry.name>.<local-domain>
    storage: # use PVC for storage
      size: 15Gi # size of PVC  

  internal-components:
    # renovate: datasource=helm depName=cert-manager
    - cert-manager: "v1.17.2"

    # renovate: datasource=helm depName=app-template
    - app-template: "4.0.1"

    # renovate: datasource=helm depName=ingress-nginx
    - nginx-ingress: "4.12.3"

    # renovate: datasource=docker depName=registry
    - registry: "3"

    # renovate: datasource=docker depName=dockurr/dnsmasq
    - dnsmasq: "2.91"

  use-service-presets: true # whether or not to use the preset values for services; leave true unless you have a good reason to override the defaults
  run-services-on-workers-only: false # whether to force application services to run only on worker nodes (when workers > 0)

  services: # additional services to deploy within the cluster 
    - name: mysql
      enabled: false # whether or not to install this component and expose the port(s)
      namespace: common-services
      ports:
        - 3306 # port to expose on the host machine
      storage: # use PVC for storage
        size: 10Gi # size of PVC
      config:
        chart: bitnami/mysql # the chart to use
        # renovate: datasource=helm depName=mysql
        version: 13.0.1
        values: # additional values to use for the chart
          auth:
            createDatabase: false # whether or not to create a database

    - name: postgres
      enabled: true # whether or not to install this component and expose the port(s)
      namespace: common-services
      ports:
        - 5432
      storage: # use PVC for storage
        size: 5Gi
      config:
        chart: bitnami/postgresql # the chart to use
        # renovate: datasource=helm depName=postgresql
        version: 16.7.10

    - name: mongodb
      enabled: false # whether or not to install this component and expose the port(s)
      namespace: common-services
      ports:
        - 27017 # port to expose on the host machine
      storage: # use PVC for storage
        size: 5Gi # size of PVC
      config:
        chart: bitnami/mongodb # the chart to use
        # renovate: datasource=helm depName=mongodb
        version: 16.5.20

    - name: rabbitmq
      enabled: false # whether or not to install this component and expose the port(s)
      namespace: common-services
      ports:
        - 5672 # port to expose on the host machine
      storage: # use PVC for storage
        size: 2Gi # size of PVC
      config:
        chart: bitnami/rabbitmq # the chart to use
        # renovate: datasource=helm depName=rabbitmq
        version: 16.0.7
        values: 
          ingress:
            enabled: true
            hostname: rmq.dev.me
            annotations:
              cert-manager.io/cluster-issuer: "mkcert-issuer"
              nginx.ingress.kubernetes.io/ssl-redirect: "true"
              nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
            tls: true

    - name: valkey
      enabled: false # whether or not to install this component and expose the port(s)
      namespace: common-services
      ports:
        - 6379 # port to expose on the host machine
      storage:
        size: 1Gi
      config:
        chart: bitnami/valkey # the chart to use
        # renovate: datasource=helm depName=valkey
        version: 3.0.9

# Example a multi-node setup with clean separation of infrastructure and application workloads:
# environment:
#   name: my-cluster
#   nodes:
#     servers: 3
#     workers: 3
#     allow-scheduling-on-control-plane: true
#     internal-components-on-control-plane: true  # cert-manager, nginx-ingress, registry on control-plane
#     labels:
#       control-plane:
#         tier: "infra"
#       worker:
#         tier: "app"
#   run-services-on-workers-only: true  # mysql, postgres, etc. on workers only
#   
#   # This creates a clean separation:
#   # - Control-plane: Kubernetes system + internal components (cert-manager, nginx-ingress, registry)
#   # - Workers: Application services (databases, message queues, etc.)